{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhIB5sXIrrLa",
        "outputId": "8900338f-64e6-4c27-991d-d384e57df603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: treys in /usr/local/lib/python3.11/dist-packages (0.1.8)\n",
            "Using device: cpu\n",
            "Starting enhanced training...\n",
            "Episode 200, Loss: 0.1622, Avg Reward: 0.054, Best: 0.054, Epsilon: 0.905, Patience: 0\n",
            "Episode 300, Loss: 0.0481, Avg Reward: 0.100, Best: 0.100, Epsilon: 0.861, Patience: 0\n",
            "Episode 400, Loss: 0.1559, Avg Reward: 0.097, Best: 0.100, Epsilon: 0.819, Patience: 1\n",
            "Episode 500, Loss: 0.0733, Avg Reward: 0.214, Best: 0.214, Epsilon: 0.779, Patience: 0\n",
            "Episode 600, Loss: 0.4834, Avg Reward: 0.208, Best: 0.214, Epsilon: 0.741, Patience: 1\n",
            "Episode 700, Loss: 0.3390, Avg Reward: 0.145, Best: 0.214, Epsilon: 0.705, Patience: 2\n",
            "Episode 800, Loss: 0.2915, Avg Reward: 0.255, Best: 0.255, Epsilon: 0.670, Patience: 0\n",
            "Episode 900, Loss: 0.4991, Avg Reward: 0.215, Best: 0.255, Epsilon: 0.638, Patience: 1\n",
            "Episode 1000, Loss: 0.5441, Avg Reward: 0.142, Best: 0.255, Epsilon: 0.606, Patience: 2\n",
            "Episode 1100, Loss: 0.6239, Avg Reward: 0.252, Best: 0.255, Epsilon: 0.577, Patience: 3\n",
            "Episode 1200, Loss: 0.8116, Avg Reward: 0.353, Best: 0.353, Epsilon: 0.549, Patience: 0\n",
            "Episode 1300, Loss: 0.6905, Avg Reward: 0.259, Best: 0.353, Epsilon: 0.522, Patience: 1\n",
            "Episode 1400, Loss: 0.4026, Avg Reward: 0.232, Best: 0.353, Epsilon: 0.496, Patience: 2\n",
            "Episode 1500, Loss: 0.5732, Avg Reward: 0.284, Best: 0.353, Epsilon: 0.472, Patience: 3\n",
            "Episode 1600, Loss: 0.7532, Avg Reward: 0.201, Best: 0.353, Epsilon: 0.449, Patience: 4\n",
            "Episode 1700, Loss: 1.6763, Avg Reward: 0.264, Best: 0.353, Epsilon: 0.427, Patience: 5\n",
            "Episode 1800, Loss: 0.7372, Avg Reward: 0.324, Best: 0.353, Epsilon: 0.406, Patience: 6\n",
            "Episode 1900, Loss: 0.6900, Avg Reward: 0.440, Best: 0.440, Epsilon: 0.387, Patience: 0\n",
            "Episode 2000, Loss: 0.9068, Avg Reward: 0.456, Best: 0.456, Epsilon: 0.368, Patience: 0\n",
            "Episode 2100, Loss: 1.3079, Avg Reward: 0.381, Best: 0.456, Epsilon: 0.350, Patience: 1\n",
            "Episode 2200, Loss: 1.0612, Avg Reward: 0.446, Best: 0.456, Epsilon: 0.333, Patience: 2\n",
            "Episode 2300, Loss: 1.1625, Avg Reward: 0.413, Best: 0.456, Epsilon: 0.317, Patience: 3\n",
            "Episode 2400, Loss: 1.1160, Avg Reward: 0.395, Best: 0.456, Epsilon: 0.301, Patience: 4\n",
            "Episode 2500, Loss: 0.6609, Avg Reward: 0.448, Best: 0.456, Epsilon: 0.286, Patience: 5\n",
            "Episode 2600, Loss: 0.5243, Avg Reward: 0.489, Best: 0.489, Epsilon: 0.272, Patience: 0\n",
            "Episode 2700, Loss: 1.3722, Avg Reward: 0.479, Best: 0.489, Epsilon: 0.259, Patience: 1\n",
            "Episode 2800, Loss: 1.1744, Avg Reward: 0.446, Best: 0.489, Epsilon: 0.247, Patience: 2\n",
            "Episode 2900, Loss: 1.4148, Avg Reward: 0.467, Best: 0.489, Epsilon: 0.234, Patience: 3\n",
            "Episode 3000, Loss: 1.3374, Avg Reward: 0.497, Best: 0.497, Epsilon: 0.223, Patience: 0\n",
            "Training completed!\n",
            "Testing enhanced model performance...\n",
            "Enhanced Test Accuracy: 92.00%\n",
            "Average Test Reward: 0.584\n",
            "Reward Std: 0.645\n",
            "\n",
            "Action Distribution:\n",
            "Fold: 6.0%\n",
            "Call: 49.1%\n",
            "Raise: 44.9%\n",
            "\n",
            "Reward Distribution:\n",
            "High rewards (>0.5): 674 (67.4%)\n",
            "Medium rewards (0-0.5): 241 (24.1%)\n",
            "Negative rewards (<0): 85 (8.5%)\n"
          ]
        }
      ],
      "source": [
        "!pip install treys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from treys import Card, Evaluator\n",
        "\n",
        "# Set device first\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enhanced poker hand evaluation with better equity calculation\n",
        "def evaluate_hand(hole_cards, community_cards, deck=None):\n",
        "    \"\"\"Evaluate hand strength and estimate equity with Monte Carlo simulation.\"\"\"\n",
        "    evaluator = Evaluator()\n",
        "\n",
        "    if len(community_cards) < 3:  # Preflop equity calculation\n",
        "        # Simple preflop hand strength based on hole cards\n",
        "        card1_str = Card.int_to_str(hole_cards[0])\n",
        "        card2_str = Card.int_to_str(hole_cards[1])\n",
        "\n",
        "        # Basic preflop hand strength scoring\n",
        "        rank1 = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A'].index(card1_str[0])\n",
        "        rank2 = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A'].index(card2_str[0])\n",
        "\n",
        "        # Pocket pair bonus\n",
        "        if rank1 == rank2:\n",
        "            equity = 0.5 + (rank1 / 26)  # Higher pairs get higher equity\n",
        "        # Suited bonus\n",
        "        elif card1_str[1] == card2_str[1]:\n",
        "            equity = 0.3 + (max(rank1, rank2) / 26) + 0.1\n",
        "        # High card strength\n",
        "        else:\n",
        "            equity = 0.2 + (max(rank1, rank2) / 26)\n",
        "\n",
        "        equity = min(equity, 0.85)  # Cap preflop equity\n",
        "        return 0, 7462, equity\n",
        "\n",
        "    # Post-flop evaluation\n",
        "    rank = evaluator.evaluate(hole_cards, community_cards)\n",
        "    rank_class = evaluator.get_rank_class(rank)\n",
        "\n",
        "    # Better equity estimation based on hand strength\n",
        "    equity_map = {\n",
        "        1: 0.95,  # Straight flush\n",
        "        2: 0.90,  # Four of a kind\n",
        "        3: 0.85,  # Full house\n",
        "        4: 0.80,  # Flush\n",
        "        5: 0.75,  # Straight\n",
        "        6: 0.65,  # Three of a kind\n",
        "        7: 0.55,  # Two pair\n",
        "        8: 0.40,  # One pair\n",
        "        9: 0.25   # High card\n",
        "    }\n",
        "\n",
        "    base_equity = equity_map.get(rank_class, 0.25)\n",
        "    # Adjust based on relative rank within class\n",
        "    relative_strength = (7462 - rank) / 7462\n",
        "    equity = base_equity + (relative_strength * 0.15)\n",
        "\n",
        "    return rank_class, rank, min(equity, 0.98)\n",
        "\n",
        "# Enhanced neural network architecture\n",
        "class PokerQNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PokerQNetwork, self).__init__()\n",
        "        # Improved card encoding\n",
        "        self.card_embedding = nn.Embedding(53, 16)  # 52 cards + padding\n",
        "\n",
        "        # Separate processing for hole cards vs community cards\n",
        "        self.hole_conv = nn.Conv1d(16, 32, kernel_size=2)  # For 2 hole cards\n",
        "        self.community_conv = nn.Conv1d(16, 32, kernel_size=3, padding=1)  # For 5 community cards\n",
        "\n",
        "        # Attention mechanism for card importance\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
        "\n",
        "        # Enhanced state processing\n",
        "        self.state_fc = nn.Linear(10, 64)\n",
        "\n",
        "        # Combined processing\n",
        "        total_features = 32 + 32 + 64  # hole + community + state\n",
        "        self.fc1 = nn.Linear(total_features, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 3)  # Fold, call, raise\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer_norm = nn.LayerNorm(total_features)\n",
        "\n",
        "    def forward(self, card_input, state_input):\n",
        "        batch_size = card_input.size(0)\n",
        "\n",
        "        # Convert card representation to embeddings\n",
        "        # Assuming card_input is [batch, 2, 7] with suit/rank encoding\n",
        "        # We'll create card IDs from this\n",
        "        card_ids = torch.zeros(batch_size, 7, dtype=torch.long, device=card_input.device)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(7):\n",
        "                if card_input[b, 0, c] > 0 or card_input[b, 1, c] > 0:  # Non-padding\n",
        "                    suit = int(card_input[b, 0, c] * 3)\n",
        "                    rank = int(card_input[b, 1, c] * 12)\n",
        "                    card_ids[b, c] = rank * 4 + suit + 1  # +1 to avoid 0 (padding)\n",
        "\n",
        "        # Embed cards\n",
        "        card_embeds = self.card_embedding(card_ids)  # [batch, 7, 16]\n",
        "\n",
        "        # Separate hole and community cards\n",
        "        hole_embeds = card_embeds[:, :2, :].transpose(1, 2)  # [batch, 16, 2]\n",
        "        community_embeds = card_embeds[:, 2:, :].transpose(1, 2)  # [batch, 16, 5]\n",
        "\n",
        "        # Process hole cards\n",
        "        hole_features = self.hole_conv(hole_embeds)  # [batch, 32, 1]\n",
        "        hole_features = hole_features.squeeze(-1)  # [batch, 32]\n",
        "\n",
        "        # Process community cards\n",
        "        community_features = self.community_conv(community_embeds)  # [batch, 32, 5]\n",
        "        community_features = F.adaptive_avg_pool1d(community_features, 1).squeeze(-1)  # [batch, 32]\n",
        "\n",
        "        # Apply attention to card features\n",
        "        combined_cards = torch.stack([hole_features, community_features], dim=1)  # [batch, 2, 32]\n",
        "        attended_cards, _ = self.attention(combined_cards, combined_cards, combined_cards)\n",
        "        attended_cards = attended_cards.mean(dim=1)  # [batch, 32]\n",
        "\n",
        "        # Process state\n",
        "        state_features = F.relu(self.state_fc(state_input))\n",
        "\n",
        "        # Combine all features\n",
        "        combined = torch.cat([hole_features, attended_cards, state_features], dim=1)\n",
        "        combined = self.layer_norm(combined)\n",
        "\n",
        "        # Forward through fully connected layers\n",
        "        x = F.relu(self.fc1(combined))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Improved game simulation with better opponent modeling\n",
        "def simulate_game(model, epsilon=0.1):\n",
        "    suits = ['h', 's', 'd', 'c']\n",
        "    ranks = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A']\n",
        "    deck = [r + s for r in ranks for s in suits]\n",
        "    random.shuffle(deck)\n",
        "\n",
        "    # Select stage with better distribution\n",
        "    stage_weights = [0.4, 0.3, 0.2, 0.1]  # More preflop scenarios\n",
        "    stage_idx = np.random.choice(4, p=stage_weights)\n",
        "    stages = ['preflop', 'flop', 'turn', 'river']\n",
        "    stage = stages[stage_idx]\n",
        "\n",
        "    # Deal cards\n",
        "    hole_cards = [Card.new(deck.pop()), Card.new(deck.pop())]\n",
        "    community_cards = []\n",
        "    if stage == 'flop':\n",
        "        community_cards = [Card.new(deck.pop()) for _ in range(3)]\n",
        "    elif stage == 'turn':\n",
        "        community_cards = [Card.new(deck.pop()) for _ in range(4)]\n",
        "    elif stage == 'river':\n",
        "        community_cards = [Card.new(deck.pop()) for _ in range(5)]\n",
        "\n",
        "    # More realistic opponent modeling\n",
        "    hand_rank, rank, equity = evaluate_hand(hole_cards, community_cards)\n",
        "\n",
        "    # Opponent behavior based on hand strength\n",
        "    if equity > 0.7:  # Strong hand\n",
        "        opponent_folded = random.random() < 0.05  # Almost never fold\n",
        "        opponent_bet = random.uniform(20, 100)\n",
        "        opponent_aggression = random.uniform(0.7, 1.0)\n",
        "    elif equity > 0.4:  # Medium hand\n",
        "        opponent_folded = random.random() < 0.3\n",
        "        opponent_bet = random.uniform(5, 40) if not opponent_folded else 0\n",
        "        opponent_aggression = random.uniform(0.3, 0.7)\n",
        "    else:  # Weak hand\n",
        "        opponent_folded = random.random() < 0.6\n",
        "        opponent_bet = random.uniform(0, 15) if not opponent_folded else 0\n",
        "        opponent_aggression = random.uniform(0.0, 0.4)\n",
        "\n",
        "    # Game state\n",
        "    pot = 10 + opponent_bet\n",
        "    player_stack = 1000\n",
        "    opponent_stack = 1000 - opponent_bet\n",
        "    player_bet = 0\n",
        "\n",
        "    # Position factor (simplified)\n",
        "    position = random.choice(['early', 'middle', 'late'])\n",
        "    position_bonus = {'early': 0.0, 'middle': 0.1, 'late': 0.2}[position]\n",
        "\n",
        "    # Encode cards properly\n",
        "    all_cards = hole_cards + community_cards\n",
        "    while len(all_cards) < 7:\n",
        "        all_cards.append(0)\n",
        "\n",
        "    card_input = np.zeros((1, 2, 7))\n",
        "\n",
        "    for i, card in enumerate(all_cards[:7]):\n",
        "        if card != 0:\n",
        "            card_str = Card.int_to_str(card)\n",
        "            card_input[0, 0, i] = suits.index(card_str[-1]) / 3.0\n",
        "            card_input[0, 1, i] = ranks.index(card_str[0]) / 12.0\n",
        "\n",
        "    card_input = torch.FloatTensor(card_input).to(device)\n",
        "\n",
        "    # Enhanced state features\n",
        "    stage_onehot = np.zeros(4)\n",
        "    stage_onehot[stage_idx] = 1\n",
        "\n",
        "    state_features = [\n",
        "        pot / 1000,\n",
        "        player_stack / 1000,\n",
        "        opponent_stack / 1000,\n",
        "        player_bet / 1000,\n",
        "        opponent_bet / 1000,\n",
        "        float(opponent_folded),\n",
        "        opponent_aggression,\n",
        "        stage_onehot[0],  # Preflop\n",
        "        stage_onehot[1],  # Flop\n",
        "        position_bonus    # Position advantage\n",
        "    ]\n",
        "\n",
        "    state_input = torch.FloatTensor([state_features]).to(device)\n",
        "\n",
        "    # Choose action\n",
        "    if random.random() < epsilon:\n",
        "        action = random.randint(0, 2)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_values = model(card_input, state_input)\n",
        "            action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "    # Much improved reward calculation\n",
        "    pot_odds = opponent_bet / (pot + opponent_bet) if opponent_bet > 0 else 0\n",
        "    base_reward = 0\n",
        "\n",
        "    if action == 0:  # Fold\n",
        "        if equity < 0.25:  # Good fold with very weak hand\n",
        "            base_reward = 0.5\n",
        "        elif equity < 0.4:  # Acceptable fold with weak hand\n",
        "            base_reward = 0.1\n",
        "        elif equity < 0.6:  # Marginal fold\n",
        "            base_reward = -0.3\n",
        "        else:  # Bad fold with strong hand\n",
        "            base_reward = -2.0\n",
        "\n",
        "    elif opponent_folded:\n",
        "        base_reward = pot / 1000 + 0.5  # Win the pot plus bonus\n",
        "\n",
        "    else:\n",
        "        if action == 1:  # Call\n",
        "            if equity > pot_odds + 0.1:  # Profitable call with margin\n",
        "                base_reward = 1.0\n",
        "            elif equity > pot_odds:  # Marginal call\n",
        "                base_reward = 0.3\n",
        "            else:  # Unprofitable call\n",
        "                base_reward = -1.0\n",
        "\n",
        "        else:  # Raise (action == 2)\n",
        "            if equity > 0.7:  # Strong hand, good aggression\n",
        "                base_reward = 1.5\n",
        "            elif equity > 0.5:  # Medium hand, reasonable aggression\n",
        "                base_reward = 0.8\n",
        "            elif equity > 0.3 and random.random() < 0.3:  # Bluff\n",
        "                base_reward = 0.4\n",
        "            else:  # Bad aggression\n",
        "                base_reward = -1.2\n",
        "\n",
        "    # Stage and position adjustments\n",
        "    stage_multiplier = [0.7, 1.0, 1.3, 1.5][stage_idx]\n",
        "    reward = base_reward * stage_multiplier + position_bonus\n",
        "\n",
        "    return card_input, state_input, action, reward\n",
        "\n",
        "# Enhanced DQN with prioritized experience replay\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.pos = 0\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
        "\n",
        "    def push(self, experience):\n",
        "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
        "\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(experience)\n",
        "        else:\n",
        "            self.buffer[self.pos] = experience\n",
        "\n",
        "        self.priorities[self.pos] = max_prio\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == self.capacity:\n",
        "            prios = self.priorities\n",
        "        else:\n",
        "            prios = self.priorities[:self.pos]\n",
        "\n",
        "        probs = prios ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        return samples, indices, np.array(weights, dtype=np.float32)\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, prio in zip(indices, priorities):\n",
        "            self.priorities[idx] = prio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Training with improved hyperparameters\n",
        "model = PokerQNetwork().to(device)\n",
        "target_model = PokerQNetwork().to(device)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "# Better optimizer settings\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=500, T_mult=2)\n",
        "\n",
        "# Use prioritized replay buffer\n",
        "replay_buffer = PrioritizedReplayBuffer(20000)\n",
        "\n",
        "# Improved hyperparameters\n",
        "gamma = 0.995\n",
        "batch_size = 128\n",
        "target_update_freq = 50\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay = 0.9995\n",
        "\n",
        "epsilon = epsilon_start\n",
        "beta = 0.4\n",
        "beta_increment = 0.0001\n",
        "\n",
        "print(\"Starting enhanced training...\")\n",
        "best_reward = -float('inf')\n",
        "patience = 0\n",
        "max_patience = 300\n",
        "\n",
        "for episode in range(3000):\n",
        "    # Decay epsilon and increase beta\n",
        "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "    beta = min(1.0, beta + beta_increment)\n",
        "\n",
        "    # Generate experience\n",
        "    card_input, state_input, action, reward = simulate_game(model, epsilon)\n",
        "\n",
        "    # Store experience\n",
        "    experience = (card_input.cpu(), state_input.cpu(), action, reward)\n",
        "    replay_buffer.push(experience)\n",
        "\n",
        "    # Train when we have enough samples\n",
        "    if len(replay_buffer) >= batch_size:\n",
        "        # Sample from prioritized buffer\n",
        "        if len(replay_buffer) >= batch_size * 2:  # Use prioritized sampling\n",
        "            batch, indices, weights = replay_buffer.sample(batch_size, beta)\n",
        "            weights = torch.FloatTensor(weights).to(device)\n",
        "        else:\n",
        "            # Fall back to uniform sampling for initial training\n",
        "            batch = random.sample(replay_buffer.buffer, batch_size)\n",
        "            indices = None\n",
        "            weights = torch.ones(batch_size).to(device)\n",
        "\n",
        "        # Prepare batch data\n",
        "        card_inputs = torch.cat([b[0] for b in batch]).to(device)\n",
        "        state_inputs = torch.cat([b[1] for b in batch]).to(device)\n",
        "        actions = torch.LongTensor([b[2] for b in batch]).to(device)\n",
        "        rewards = torch.FloatTensor([b[3] for b in batch]).to(device)\n",
        "\n",
        "        # Compute Q-values\n",
        "        optimizer.zero_grad()\n",
        "        current_q_values = model(card_inputs, state_inputs)\n",
        "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Target Q-values (using target network)\n",
        "        with torch.no_grad():\n",
        "            target_q_values = target_model(card_inputs, state_inputs)\n",
        "            max_target_q = target_q_values.max(1)[0]\n",
        "            target_values = rewards + gamma * max_target_q\n",
        "\n",
        "        # Compute weighted loss\n",
        "        td_errors = target_values - current_q_values\n",
        "        loss = (weights * td_errors.pow(2)).mean()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update priorities if using prioritized replay\n",
        "        if indices is not None:\n",
        "            priorities = td_errors.abs().detach().cpu().numpy() + 1e-6\n",
        "            replay_buffer.update_priorities(indices, priorities)\n",
        "\n",
        "        # Update target network\n",
        "        if episode % target_update_freq == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        # Enhanced logging and early stopping\n",
        "        if episode % 100 == 99:\n",
        "            recent_rewards = [exp[3] for exp in list(replay_buffer.buffer)[-200:]]\n",
        "            avg_reward = np.mean(recent_rewards)\n",
        "\n",
        "            if avg_reward > best_reward:\n",
        "                best_reward = avg_reward\n",
        "                patience = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), 'best_poker_model.pth')\n",
        "            else:\n",
        "                patience += 1\n",
        "\n",
        "            print(f\"Episode {episode + 1}, Loss: {loss.item():.4f}, \"\n",
        "                  f\"Avg Reward: {avg_reward:.3f}, Best: {best_reward:.3f}, \"\n",
        "                  f\"Epsilon: {epsilon:.3f}, Patience: {patience}\")\n",
        "\n",
        "            if patience >= max_patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Load best model for testing\n",
        "model.load_state_dict(torch.load('best_poker_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Comprehensive testing\n",
        "print(\"Testing enhanced model performance...\")\n",
        "test_results = {'correct': 0, 'total': 0, 'rewards': [], 'actions': []}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(1000):  # More comprehensive testing\n",
        "        card_input, state_input, action, reward = simulate_game(model, epsilon=0.0)\n",
        "        test_results['rewards'].append(reward)\n",
        "        test_results['actions'].append(action)\n",
        "\n",
        "        # Improved accuracy calculation\n",
        "        if reward > 0.3:  # Good decision\n",
        "            test_results['correct'] += 1\n",
        "        elif reward > -0.2:  # Neutral/acceptable decision\n",
        "            test_results['correct'] += 1\n",
        "        # else: bad decision, don't count as correct\n",
        "\n",
        "        test_results['total'] += 1\n",
        "\n",
        "accuracy = 100 * test_results['correct'] / test_results['total']\n",
        "avg_reward = np.mean(test_results['rewards'])\n",
        "reward_std = np.std(test_results['rewards'])\n",
        "\n",
        "print(f\"Enhanced Test Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Average Test Reward: {avg_reward:.3f}\")\n",
        "print(f\"Reward Std: {reward_std:.3f}\")\n",
        "\n",
        "# Detailed action analysis\n",
        "action_counts = [0, 0, 0]\n",
        "for action in test_results['actions']:\n",
        "    action_counts[action] += 1\n",
        "\n",
        "total_actions = sum(action_counts)\n",
        "print(f\"\\nAction Distribution:\")\n",
        "print(f\"Fold: {100*action_counts[0]/total_actions:.1f}%\")\n",
        "print(f\"Call: {100*action_counts[1]/total_actions:.1f}%\")\n",
        "print(f\"Raise: {100*action_counts[2]/total_actions:.1f}%\")\n",
        "\n",
        "# Performance by reward range\n",
        "high_rewards = [r for r in test_results['rewards'] if r > 0.5]\n",
        "medium_rewards = [r for r in test_results['rewards'] if 0 <= r <= 0.5]\n",
        "low_rewards = [r for r in test_results['rewards'] if r < 0]\n",
        "\n",
        "print(f\"\\nReward Distribution:\")\n",
        "print(f\"High rewards (>0.5): {len(high_rewards)} ({100*len(high_rewards)/len(test_results['rewards']):.1f}%)\")\n",
        "print(f\"Medium rewards (0-0.5): {len(medium_rewards)} ({100*len(medium_rewards)/len(test_results['rewards']):.1f}%)\")\n",
        "print(f\"Negative rewards (<0): {len(low_rewards)} ({100*len(low_rewards)/len(test_results['rewards']):.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p streamlit_app"
      ],
      "metadata": {
        "id": "cWszne4uL2_d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app/app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from treys import Card, Evaluator\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Define PokerQNetwork class (original with 10 state features)\n",
        "class PokerQNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PokerQNetwork, self).__init__()\n",
        "        self.card_embedding = nn.Embedding(53, 16)\n",
        "        self.hole_conv = nn.Conv1d(16, 32, kernel_size=2)\n",
        "        self.community_conv = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
        "        self.state_fc = nn.Linear(10, 64)  # Original state input size\n",
        "        total_features = 32 + 32 + 64\n",
        "        self.fc1 = nn.Linear(total_features, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 3)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.layer_norm = nn.LayerNorm(total_features)\n",
        "\n",
        "    def forward(self, card_input, state_input):\n",
        "        batch_size = card_input.size(0)\n",
        "        card_ids = torch.zeros(batch_size, 7, dtype=torch.long, device=card_input.device)\n",
        "\n",
        "        suits = ['h', 's', 'd', 'c']\n",
        "        ranks = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A']\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c in range(7):\n",
        "                if card_input[b, 0, c] > 0 or card_input[b, 1, c] > 0:\n",
        "                    suit = int(card_input[b, 0, c] * 3)\n",
        "                    rank = int(card_input[b, 1, c] * 12)\n",
        "                    card_ids[b, c] = rank * 4 + suit + 1\n",
        "\n",
        "        card_embeds = self.card_embedding(card_ids)\n",
        "        hole_embeds = card_embeds[:, :2, :].transpose(1, 2)\n",
        "        community_embeds = card_embeds[:, 2:, :].transpose(1, 2)\n",
        "\n",
        "        hole_features = self.hole_conv(hole_embeds).squeeze(-1)\n",
        "        community_features = self.community_conv(community_embeds)\n",
        "        community_features = torch.nn.functional.adaptive_avg_pool1d(community_features, 1).squeeze(-1)\n",
        "\n",
        "        combined_cards = torch.stack([hole_features, community_features], dim=1)\n",
        "        attended_cards, _ = self.attention(combined_cards, combined_cards, combined_cards)\n",
        "        attended_cards = attended_cards.mean(dim=1)\n",
        "\n",
        "        state_features = torch.nn.functional.relu(self.state_fc(state_input))\n",
        "\n",
        "        combined = torch.cat([hole_features, attended_cards, state_features], dim=1)\n",
        "        combined = self.layer_norm(combined)\n",
        "\n",
        "        x = torch.nn.functional.relu(self.fc1(combined))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.nn.functional.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Enhanced evaluate_hand function with multi-opponent adjustment\n",
        "def evaluate_hand(hole_cards, community_cards, num_opponents=1):\n",
        "    evaluator = Evaluator()\n",
        "\n",
        "    if len(community_cards) < 3:  # Preflop\n",
        "        card1_str = Card.int_to_str(hole_cards[0])\n",
        "        card2_str = Card.int_to_str(hole_cards[1])\n",
        "        rank1 = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A'].index(card1_str[0])\n",
        "        rank2 = ['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A'].index(card2_str[0])\n",
        "\n",
        "        if rank1 == rank2:\n",
        "            equity = 0.5 + (rank1 / 26)\n",
        "        elif card1_str[1] == card2_str[1]:\n",
        "            equity = 0.3 + (max(rank1, rank2) / 26) + 0.1\n",
        "        else:\n",
        "            equity = 0.2 + (max(rank1, rank2) / 26)\n",
        "\n",
        "        # Adjust equity for multiple opponents\n",
        "        equity = equity * (0.85 ** num_opponents)\n",
        "        equity = min(equity, 0.85)\n",
        "\n",
        "        # Hand strength percentile\n",
        "        all_hands = 1326\n",
        "        rank = min(rank1, rank2) if rank1 != rank2 else rank1\n",
        "        suited = card1_str[1] == card2_str[1]\n",
        "        if rank1 == rank2:\n",
        "            percentile = 100 * (1 - (6 * (12 - rank) / all_hands))\n",
        "        elif suited:\n",
        "            percentile = 100 * (1 - (4 * (169 - (rank1 + rank2 + 10)) / all_hands))\n",
        "        else:\n",
        "            percentile = 100 * (1 - (12 * (169 - (rank1 + rank2)) / all_hands))\n",
        "\n",
        "        return 0, 7462, equity, max(0, min(100, percentile))\n",
        "\n",
        "    # Post-flop\n",
        "    rank = evaluator.evaluate(hole_cards, community_cards)\n",
        "    rank_class = evaluator.get_rank_class(rank)\n",
        "\n",
        "    equity_map = {\n",
        "        1: 0.95, 2: 0.90, 3: 0.85, 4: 0.80, 5: 0.75,\n",
        "        6: 0.65, 7: 0.55, 8: 0.40, 9: 0.25\n",
        "    }\n",
        "\n",
        "    base_equity = equity_map.get(rank_class, 0.25)\n",
        "    relative_strength = (7462 - rank) / 7462\n",
        "    equity = base_equity + (relative_strength * 0.15)\n",
        "\n",
        "    # Adjust for multiple opponents\n",
        "    equity = equity * (0.9 ** num_opponents)\n",
        "    equity = min(equity, 0.98)\n",
        "\n",
        "    # Hand strength percentile\n",
        "    percentile = 100 * (1 - rank / 7462)\n",
        "\n",
        "    return rank_class, rank, equity, max(0, min(100, percentile))\n",
        "\n",
        "# Set up the Streamlit page\n",
        "st.set_page_config(page_title=\"Advanced Poker AI Predictor\", page_icon=\"🃏\", layout=\"wide\")\n",
        "st.title('Advanced Poker AI Predictor')\n",
        "st.write('This app uses a deep Q-learning model to predict the best action in Texas Hold\\'em, supporting multiple opponents, visual card selection, and detailed analysis.')\n",
        "\n",
        "# Initialize session state for prediction history\n",
        "if 'prediction_history' not in st.session_state:\n",
        "    st.session_state.prediction_history = []\n",
        "\n",
        "# Load the model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PokerQNetwork().to(device)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('best_poker_model.pth', map_location=device))\n",
        "    model.eval()\n",
        "    st.success(\"Model loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    st.error(\"Model file 'best_poker_model.pth' not found. Please ensure it's in the same directory.\")\n",
        "    st.stop()\n",
        "\n",
        "# Sidebar for user inputs\n",
        "st.sidebar.header('Game State Inputs')\n",
        "\n",
        "# Number of opponents\n",
        "num_opponents = st.sidebar.selectbox('Number of Opponents', [1, 2, 3, 4, 5], index=0)\n",
        "\n",
        "# Visual card picker\n",
        "st.sidebar.subheader('Card Selection')\n",
        "suits = ['♥', '♠', '♦', '♣']\n",
        "ranks = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']\n",
        "deck = [f\"{r}{s}\" for r in ranks for s in suits]\n",
        "\n",
        "def card_image(card_str):\n",
        "    if card_str == \"None\":\n",
        "        return None\n",
        "    rank = card_str[:-1].lower().replace('10', 'T').replace('j', 'J').replace('q', 'Q').replace('k', 'K').replace('a', 'A')\n",
        "    suit = {'♥': 'H', '♠': 'S', '♦': 'D', '♣': 'C'}[card_str[-1]]\n",
        "    image_url = f'https://deckofcardsapi.com/static/img/{rank}{suit}.png'\n",
        "    return image_url\n",
        "\n",
        "# Hole cards\n",
        "st.sidebar.write('**Hole Cards**')\n",
        "hole_cols = st.sidebar.columns(2)\n",
        "hole_cards = []\n",
        "selected_cards = []\n",
        "for i in range(2):\n",
        "    with hole_cols[i]:\n",
        "        card_key = f\"hole_card_{i}\"\n",
        "        selected = st.selectbox(f\"Card {i+1}\", ['None'] + deck, key=card_key)\n",
        "        if selected != 'None':\n",
        "            hole_cards.append(Card.new(selected[:-1] + {'♥': 'h', '♠': 's', '♦': 'd', '♣': 'c'}[selected[-1]]))\n",
        "            selected_cards.append(selected)\n",
        "            st.image(card_image(selected), width=80)\n",
        "\n",
        "# Community cards\n",
        "st.sidebar.subheader('Community Cards')\n",
        "stage = st.sidebar.selectbox('Game Stage', ['Preflop', 'Flop', 'Turn', 'River'])\n",
        "community_cards = []\n",
        "if stage != 'Preflop':\n",
        "    num_comm_cards = 3 if stage == 'Flop' else 4 if stage == 'Turn' else 5\n",
        "    comm_cols = st.sidebar.columns(num_comm_cards)\n",
        "    for i in range(num_comm_cards):\n",
        "        with comm_cols[i]:\n",
        "            card_key = f\"comm_card_{i}\"\n",
        "            selected = st.selectbox(f\"Card {i+1}\", ['None'] + deck, key=card_key)\n",
        "            if selected != 'None':\n",
        "                community_cards.append(Card.new(selected[:-1] + {'♥': 'h', '♠': 's', '♦': 'd', '♣': 'c'}[selected[-1]]))\n",
        "                selected_cards.append(selected)\n",
        "                st.image(card_image(selected), width=80)\n",
        "\n",
        "# Validate card selections\n",
        "if len(selected_cards) != len(set(selected_cards)):\n",
        "    st.error(\"Duplicate cards selected. Please choose unique cards.\")\n",
        "    st.stop()\n",
        "\n",
        "if len(hole_cards) != 2:\n",
        "    st.error(\"Please select both hole cards.\")\n",
        "    st.stop()\n",
        "\n",
        "# Game state inputs for player and opponents\n",
        "st.sidebar.subheader('Game State')\n",
        "pot = st.sidebar.slider('Pot Size ($)', 0, 5000, 50)\n",
        "player_stack = st.sidebar.slider('Your Stack ($)', 0, 10000, 1000)\n",
        "player_bet = st.sidebar.slider('Your Current Bet ($)', 0, 1000, 0)\n",
        "position = st.sidebar.selectbox('Position', ['Early', 'Middle', 'Late'])\n",
        "\n",
        "opponent_data = []\n",
        "for opp in range(num_opponents):\n",
        "    with st.sidebar.expander(f\"Opponent {opp+1}\"):\n",
        "        opp_stack = st.slider(f'Opponent {opp+1} Stack ($)', 0, 10000, 1000, key=f'opp_stack_{opp}')\n",
        "        opp_bet = st.slider(f'Opponent {opp+1} Bet ($)', 0, 1000, 10, key=f'opp_bet_{opp}')\n",
        "        opp_folded = st.checkbox(f'Opponent {opp+1} Folded', value=False, key=f'opp_folded_{opp}')\n",
        "        opp_aggression = st.slider(f'Opponent {opp+1} Aggression (0-1)', 0.0, 1.0, 0.5, key=f'opp_agg_{opp}')\n",
        "        opponent_data.append({\n",
        "            'stack': opp_stack,\n",
        "            'bet': opp_bet,\n",
        "            'folded': opp_folded,\n",
        "            'aggression': opp_aggression\n",
        "        })\n",
        "\n",
        "# Prepare input tensors\n",
        "card_input = np.zeros((1, 2, 7))\n",
        "suits_map = {'h': 0, 's': 1, 'd': 2, 'c': 3}\n",
        "ranks_map = {r: i for i, r in enumerate(['2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A'])}\n",
        "all_cards = hole_cards + community_cards\n",
        "while len(all_cards) < 7:\n",
        "    all_cards.append(0)\n",
        "\n",
        "for i, card in enumerate(all_cards[:7]):\n",
        "    if card != 0:\n",
        "        card_str = Card.int_to_str(card)\n",
        "        card_input[0, 0, i] = suits_map[card_str[-1]] / 3.0\n",
        "        card_input[0, 1, i] = ranks_map[card_str[0]] / 12.0\n",
        "\n",
        "card_input = torch.FloatTensor(card_input).to(device)\n",
        "\n",
        "# Encode state (adjusted to 10 features)\n",
        "stage_onehot = np.zeros(4)\n",
        "stage_idx = ['Preflop', 'Flop', 'Turn', 'River'].index(stage)\n",
        "stage_onehot[stage_idx] = 1\n",
        "\n",
        "position_bonus = {'Early': 0.0, 'Middle': 0.1, 'Late': 0.2}[position]\n",
        "\n",
        "# Aggregate opponent data\n",
        "avg_opp_stack = sum(opp['stack'] for opp in opponent_data) / max(1, num_opponents)\n",
        "avg_opp_bet = sum(opp['bet'] for opp in opponent_data if not opp['folded']) / max(1, sum(1 for opp in opponent_data if not opp['folded']))\n",
        "avg_opp_aggression = sum(opp['aggression'] for opp in opponent_data) / max(1, num_opponents)\n",
        "any_opp_folded = any(opp['folded'] for opp in opponent_data)\n",
        "\n",
        "state_features = [\n",
        "    pot / 1000,\n",
        "    player_stack / 1000,\n",
        "    avg_opp_stack / 1000,\n",
        "    player_bet / 1000,\n",
        "    avg_opp_bet / 1000,\n",
        "    float(any_opp_folded),\n",
        "    avg_opp_aggression,\n",
        "    stage_onehot[0],\n",
        "    stage_onehot[1],\n",
        "    position_bonus\n",
        "]\n",
        "\n",
        "state_input = torch.FloatTensor([state_features]).to(device)\n",
        "\n",
        "# Display user inputs\n",
        "st.subheader('Your Inputs')\n",
        "col1, col2 = st.columns([2, 1])\n",
        "with col1:\n",
        "    st.write('**Hole Cards:**', ', '.join([Card.int_to_pretty_str(c) for c in hole_cards]))\n",
        "    st.write('**Community Cards:**', ', '.join([Card.int_to_pretty_str(c) for c in community_cards]) if community_cards else 'None')\n",
        "    st.write('**Game State:**')\n",
        "    input_data = {\n",
        "        'Pot Size ($)': pot,\n",
        "        'Your Stack ($)': player_stack,\n",
        "        'Your Bet ($)': player_bet,\n",
        "        'Avg Opponent Stack ($)': avg_opp_stack,\n",
        "        'Avg Opponent Bet ($)': avg_opp_bet,\n",
        "        'Any Opponent Folded': any_opp_folded,\n",
        "        'Avg Opponent Aggression': avg_opp_aggression,\n",
        "        'Game Stage': stage,\n",
        "        'Position': position,\n",
        "        'Number of Opponents': num_opponents\n",
        "    }\n",
        "    st.write(pd.DataFrame([input_data]))\n",
        "\n",
        "# Display cards\n",
        "with col2:\n",
        "    st.write('**Your Cards**')\n",
        "    cols = st.columns(2)\n",
        "    for i, card in enumerate(hole_cards):\n",
        "        cols[i].image(card_image(Card.int_to_pretty_str(card)), caption=Card.int_to_pretty_str(card), width=80)\n",
        "    if community_cards:\n",
        "        st.write('**Community Cards**')\n",
        "        cols = st.columns(len(community_cards))\n",
        "        for i, card in enumerate(community_cards):\n",
        "            cols[i].image(card_image(Card.int_to_pretty_str(card)), caption=Card.int_to_pretty_str(card), width=80)\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    q_values = model(card_input, state_input)\n",
        "    action_probs = torch.softmax(q_values, dim=1).cpu().numpy()[0]\n",
        "    action = torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "# Evaluate hand strength\n",
        "rank_class, rank, equity, percentile = evaluate_hand(hole_cards, community_cards, num_opponents)\n",
        "\n",
        "# Store prediction in history\n",
        "prediction_entry = {\n",
        "    'Timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'Hole Cards': ', '.join([Card.int_to_pretty_str(c) for c in hole_cards]),\n",
        "    'Community Cards': ', '.join([Card.int_to_pretty_str(c) for c in community_cards]) if community_cards else 'None',\n",
        "    'Stage': stage,\n",
        "    'Recommended Action': ['Fold', 'Call', 'Raise'][action],\n",
        "    'Equity': f'{equity:.2%}',\n",
        "    'Hand Strength Percentile': f'{percentile:.1f}%'\n",
        "}\n",
        "st.session_state.prediction_history.append(prediction_entry)\n",
        "if len(st.session_state.prediction_history) > 10:\n",
        "    st.session_state.prediction_history = st.session_state.prediction_history[-10:]\n",
        "\n",
        "# Display results\n",
        "st.subheader('Prediction')\n",
        "action_names = ['Fold', 'Call', 'Raise']\n",
        "st.write(f'The model recommends to **{action_names[action]}**.')\n",
        "st.write(f'Estimated Hand Equity: **{equity:.2%}** (against {num_opponents} opponent(s))')\n",
        "st.write(f'Hand Strength Percentile: **{percentile:.1f}%** (top {100-percentile:.1f}% of possible hands)')\n",
        "\n",
        "# Action probabilities\n",
        "st.subheader('Action Probabilities')\n",
        "prob_df = pd.DataFrame({\n",
        "    'Action': action_names,\n",
        "    'Probability': [f'{p:.2%}' for p in action_probs]\n",
        "})\n",
        "st.write(prob_df)\n",
        "\n",
        "fig = px.bar(\n",
        "    x=action_names,\n",
        "    y=action_probs,\n",
        "    title='Action Probability Distribution',\n",
        "    labels={'x': 'Action', 'y': 'Probability'},\n",
        "    text=[f'{p:.2%}' for p in action_probs]\n",
        ")\n",
        "fig.update_traces(textposition='auto')\n",
        "fig.update_yaxes(range=[0, 1], tickformat='.0%')\n",
        "st.plotly_chart(fig)\n",
        "\n",
        "# In-depth analysis\n",
        "st.subheader('In-Depth Analysis')\n",
        "pot_odds = avg_opp_bet / (pot + avg_opp_bet) if avg_opp_bet > 0 else 0\n",
        "st.write(f'**Pot Odds**: {pot_odds:.2%} (you need {pot_odds:.2%} equity to break even on a call)')\n",
        "\n",
        "# Stage-specific insights\n",
        "stage_insights = {\n",
        "    'Preflop': f\"In the preflop stage, your hand's equity ({equity:.2%}) is based on starting hand strength against {num_opponents} opponent(s). With a percentile of {percentile:.1f}%, your hand is {'strong' if percentile > 80 else 'medium' if percentile > 50 else 'weak'}. {'Consider raising with strong hands to build the pot.' if percentile > 80 else 'Play cautiously unless position or odds favor you.'}\",\n",
        "    'Flop': f\"On the flop, your equity ({equity:.2%}) reflects your hand's strength with three community cards. A percentile of {percentile:.1f}% indicates {'a strong made hand or draw' if percentile > 70 else 'a moderate hand' if percentile > 40 else 'a weak hand or weak draw'}. {'Aggression may be warranted with strong hands or draws.' if percentile > 70 else 'Evaluate draws carefully against pot odds.'}\",\n",
        "    'Turn': f\"On the turn, with four community cards, your equity ({equity:.2%}) is more defined. Your hand's percentile ({percentile:.1f}%) suggests {'a strong hand or draw' if percentile > 65 else 'a marginal hand' if percentile > 35 else 'a weak hand'}. {'Protect strong hands with bets; consider folding weak hands unless odds are favorable.' if percentile > 65 else 'Be cautious with marginal hands.'}\",\n",
        "    'River': f\"On the river, your hand is fully defined with equity ({equity:.2%}) and percentile ({percentile:.1f}%). This indicates {'a strong hand' if percentile > 60 else 'a medium hand' if percentile > 30 else 'a weak hand'}. {'Value bet strong hands; bluff selectively with weak hands.' if percentile > 60 else 'Check or fold unless pot odds justify a call.'}\"\n",
        "}\n",
        "st.write(f'**Stage Insight ({stage})**: {stage_insights[stage]}')\n",
        "\n",
        "# Action analysis\n",
        "st.write('**Action Analysis**:')\n",
        "for i, action_name in enumerate(action_names):\n",
        "    expected_value = 0\n",
        "    if action_name == 'Fold':\n",
        "        expected_value = 0\n",
        "        analysis = \"Folding avoids further risk but forfeits the current pot. Recommended with weak hands or poor pot odds.\"\n",
        "    elif action_name == 'Call':\n",
        "        if equity > pot_odds + 0.1:\n",
        "            expected_value = (equity * pot) - ((1 - equity) * avg_opp_bet)\n",
        "            analysis = f\"Calling is profitable with {equity:.2%} equity against {pot_odds:.2%} pot odds. Expected value: ${expected_value:.2f}.\"\n",
        "        elif equity > pot_odds:\n",
        "            expected_value = (equity * pot) - ((1 - equity) * avg_opp_bet)\n",
        "            analysis = f\"Calling is marginal with {equity:.2%} equity close to {pot_odds:.2%} pot odds. Expected value: ${expected_value:.2f}.\"\n",
        "        else:\n",
        "            expected_value = (equity * pot) - ((1 - equity) * avg_opp_bet)\n",
        "            analysis = f\"Calling may be unprofitable with {equity:.2%} equity below {pot_odds:.2%} pot odds. Expected value: ${expected_value:.2f}.\"\n",
        "    else:  # Raise\n",
        "        raise_amount = avg_opp_bet * 2 if avg_opp_bet > 0 else 20\n",
        "        if equity > 0.6:\n",
        "            expected_value = (equity * (pot + raise_amount)) - ((1 - equity) * (avg_opp_bet + raise_amount))\n",
        "            analysis = f\"Raising with a strong hand ({equity:.2%} equity) can build the pot or force folds. Expected value: ${expected_value:.2f}.\"\n",
        "        elif equity > 0.4 and avg_opp_aggression < 0.5:\n",
        "            expected_value = (equity * (pot + raise_amount)) - ((1 - equity) * (avg_opp_bet + raise_amount))\n",
        "            analysis = f\"Raising as a semi-bluff with {equity:.2%} equity may induce folds from less aggressive opponents. Expected value: ${expected_value:.2f}.\"\n",
        "        else:\n",
        "            expected_value = (equity * (pot + raise_amount)) - ((1 - equity) * (avg_opp_bet + raise_amount))\n",
        "            analysis = f\"Raising with {equity:.2%} equity is risky against aggressive opponents. Expected value: ${expected_value:.2f}.\"\n",
        "\n",
        "    st.write(f\"- **{action_name}**: {analysis}\")\n",
        "\n",
        "# Prediction history\n",
        "st.subheader('Prediction History (Last 10)')\n",
        "if st.session_state.prediction_history:\n",
        "    history_df = pd.DataFrame(st.session_state.prediction_history)\n",
        "    st.write(history_df)\n",
        "else:\n",
        "    st.write(\"No predictions yet.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3TRh7UeLw5o",
        "outputId": "25057d97-02ea-4b1a-bed5-3290287dd5a7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit treys\n",
        "!npm install localtunnel\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "!streamlit run streamlit_app/app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_YBL562L-E2",
        "outputId": "2617e902-e60a-41d4-fbdb-11f24da59cd6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 2s\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K34.168.235.31\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.168.235.31:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://four-clubs-attack.loca.lt\n",
            "2025-06-13 22:41:07.798 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}